{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLoLpOvqfe_U"
   },
   "source": [
    "# Easy Object Detection with Language Modeling: Simple Implementation of Pix2Seq model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4zdDPPghKIl"
   },
   "source": [
    "![](https://raw.githubusercontent.com/moein-shariatnia/Pix2Seq/master/imgs/pix2seq%20-%20framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DLsYm_ufiMD"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iXKRgKXfuy5"
   },
   "source": [
    "Once I first heard about the paper \"[Pix2seq: A Language Modeling Framework for Object Detection](https://arxiv.org/abs/2109.10852)\" , I got pretty damn excited and I was sure my next blog post will be about it; so, here I am writing this post and hoping that you'll like it and find the pix2seq model easy to understand and implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yzotDyzgsLu"
   },
   "source": [
    "## What's interesting about this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfluGvr1gsJb"
   },
   "source": [
    "The idea is pretty simple: Reframe the object detection problem as a task of text (token) generation! We want the model to \"tell us\" what objects exist in the image and also the (x, y) coordinates of their bounding boxes (bboxes), all in a specific format in the generated sequence; just like text generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEEUl5CegsHV"
   },
   "source": [
    "![](https://raw.githubusercontent.com/moein-shariatnia/Pix2Seq/master/imgs/pix2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyESfT6Bg0Fv"
   },
   "source": [
    "As you see, the object detection task is transformed to an image-captioning-ish task: describe the image in text (sequence) but this time tell us exactly where the objects are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1ccBWwng315"
   },
   "source": [
    "# Pix2Seq: Simple Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEnPYb_Hg5pg"
   },
   "source": [
    "## Needed Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOZxD31kg9O3"
   },
   "source": [
    "The closest task to what Pix2Seq does is image-captioning. So, we are going to need an image encoder to convert an image into vectors of hidden representation and then a decoder to take the image representations and those of the previously generated tokens and predict the next token. We also need a tokenizer to convert object classes and coordinates into tokens that form their special vocabulary; just like the words in a natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzBX6FJWhBG9"
   },
   "source": [
    "## My Simple Implementation of Pix2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHFV1ryfhBFF"
   },
   "source": [
    "![](https://raw.githubusercontent.com/moein-shariatnia/Pix2Seq/master/imgs/pix2seq%20-%20framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26qsPVWXhDeg"
   },
   "source": [
    "You can see the high level pipeline of this project in the picture above. As you see, we need a dataset of images and their bboxes for which we will use Pascal VOC 2012 dataset. Next, we will write our own tokenizer from scratch to convert the bbox classes and coordinates into a sequence of tokens. Then, we will use DeiT [(from this paper)](https://arxiv.org/abs/2012.12877) as our image encoder and feed the image embeddings to a vanilla Transformer Decoder [(from this paper)](https://arxiv.org/abs/1706.03762?amp=1). The decoder's task is to predict the next token given the previous ones. The outputs of the decoder are given to the language modeling loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMDy77nHb6z5"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhjOTjXOU96Z",
    "outputId": "521222ba-6563-4d61-b62c-eff4a64bde63"
   },
   "outputs": [],
   "source": [
    "# !pip install timm -q\n",
    "# !pip install transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGNZ8wMqb-Cn"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:13.466398Z",
     "iopub.status.busy": "2022-08-19T12:52:13.465925Z",
     "iopub.status.idle": "2022-08-19T12:52:19.217498Z",
     "shell.execute_reply": "2022-08-19T12:52:19.216447Z",
     "shell.execute_reply.started": "2022-08-19T12:52:13.466291Z"
    },
    "id": "0YTPtqoyVLSC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-08 17:04:05.281750: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-08 17:04:05.307186: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 17:04:05.660497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import timm\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "import transformers\n",
    "from transformers import top_k_top_p_filtering\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:19.225468Z",
     "iopub.status.busy": "2022-08-19T12:52:19.222838Z",
     "iopub.status.idle": "2022-08-19T12:52:19.235024Z",
     "shell.execute_reply": "2022-08-19T12:52:19.234201Z",
     "shell.execute_reply.started": "2022-08-19T12:52:19.225425Z"
    },
    "id": "-WeV2AJhLAZW"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P90ymvaLAZW"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEmUqskUhkdu"
   },
   "source": [
    "This will be where we store the most important variables in order to have a quick access to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:43.100748Z",
     "iopub.status.busy": "2022-08-19T12:52:43.100336Z",
     "iopub.status.idle": "2022-08-19T12:52:43.107952Z",
     "shell.execute_reply": "2022-08-19T12:52:43.105880Z",
     "shell.execute_reply.started": "2022-08-19T12:52:43.100716Z"
    },
    "id": "l8RcBmvpLAZX"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    img_path = \"datasets/VOCdevkit/VOC2012/JPEGImages\"\n",
    "    xml_path = \"datasets/VOCdevkit/VOC2012/Annotations\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    max_len = 300\n",
    "    img_size = 384\n",
    "    num_bins = img_size\n",
    "    \n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    \n",
    "    model_name = 'deit3_small_patch16_384_in21ft1k'\n",
    "    num_patches = 576\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    generation_steps = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRnFmDt7LAZX"
   },
   "source": [
    "# Download and Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQysdnGOLZOd",
    "outputId": "a1e7d210-185e-4485-9638-c657b44e3b37"
   },
   "outputs": [],
   "source": [
    "# !wget -O \"./content/trainval.tar\" \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zhKb1BBCVVJK"
   },
   "outputs": [],
   "source": [
    "# !tar -xvf \"./content/trainval.tar\" > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:19.303081Z",
     "iopub.status.busy": "2022-08-19T12:52:19.302631Z",
     "iopub.status.idle": "2022-08-19T12:52:19.481680Z",
     "shell.execute_reply": "2022-08-19T12:52:19.480715Z",
     "shell.execute_reply.started": "2022-08-19T12:52:19.303008Z"
    },
    "id": "Q6tq3xWBX96L",
    "outputId": "548b65a0-aab9-48df-cde9-ee442fa04151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17125, 17125)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_FILES = glob(CFG.img_path + \"/*.jpg\")\n",
    "XML_FILES = glob(CFG.xml_path + \"/*.xml\")\n",
    "len(XML_FILES), len(IMG_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAHih3vYcHs6"
   },
   "source": [
    "## Process XML files and build the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:20.380216Z",
     "iopub.status.busy": "2022-08-19T12:52:20.379856Z",
     "iopub.status.idle": "2022-08-19T12:52:20.400516Z",
     "shell.execute_reply": "2022-08-19T12:52:20.399440Z",
     "shell.execute_reply.started": "2022-08-19T12:52:20.380185Z"
    },
    "id": "LfCeQoGlYXix"
   },
   "outputs": [],
   "source": [
    "class XMLParser:\n",
    "    def __init__(self,xml_file):\n",
    "\n",
    "        self.xml_file = xml_file\n",
    "        self._root = ET.parse(self.xml_file).getroot()\n",
    "        self._objects = self._root.findall(\"object\")\n",
    "        # path to the image file as describe in the xml file\n",
    "        self.img_path = os.path.join(CFG.img_path, self._root.find('filename').text)\n",
    "        # image id \n",
    "        self.image_id = self._root.find(\"filename\").text\n",
    "        # names of the classes contained in the xml file\n",
    "        self.names = self._get_names()\n",
    "        # coordinates of the bounding boxes\n",
    "        self.boxes = self._get_bndbox()\n",
    "#         print(self.names, self.boxes)\n",
    "\n",
    "    def parse_xml(self):\n",
    "        \"\"\"\"Parse the xml file returning the root.\"\"\"\n",
    "    \n",
    "        tree = ET.parse(self.xml_file)\n",
    "        return tree.getroot()\n",
    "\n",
    "    def _get_names(self):\n",
    "\n",
    "        names = []\n",
    "        for obj in self._objects:\n",
    "            name = obj.find(\"name\")\n",
    "            names.append(name.text)\n",
    "\n",
    "        return np.array(names)\n",
    "\n",
    "    def _get_bndbox(self):\n",
    "\n",
    "        boxes = []\n",
    "        for obj in self._objects:\n",
    "            coordinates = []\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            coordinates.append(np.int32(bndbox.find(\"xmin\").text))\n",
    "            coordinates.append(np.int32(np.float32(bndbox.find(\"ymin\").text)))\n",
    "            coordinates.append(np.int32(bndbox.find(\"xmax\").text))\n",
    "            coordinates.append(np.int32(bndbox.find(\"ymax\").text))\n",
    "            boxes.append(coordinates)\n",
    "\n",
    "        return np.array(boxes)\n",
    "\n",
    "def xml_files_to_df(xml_files):\n",
    "    \n",
    "    \"\"\"\"Return pandas dataframe from list of XML files.\"\"\"\n",
    "    \n",
    "    names = []\n",
    "    boxes = []\n",
    "    image_id = []\n",
    "    xml_path = []\n",
    "    img_path = []\n",
    "    for f in xml_files:\n",
    "        xml = XMLParser(f)\n",
    "        names.extend(xml.names)\n",
    "        boxes.extend(xml.boxes)\n",
    "        image_id.extend([xml.image_id] * len(xml.names))\n",
    "        xml_path.extend([xml.xml_file] * len(xml.names))\n",
    "        img_path.extend([xml.img_path] * len(xml.names))\n",
    "    a = {\"image_id\": image_id,\n",
    "         \"names\": names,\n",
    "         \"boxes\": boxes,\n",
    "         \"xml_path\":xml_path,\n",
    "         \"img_path\":img_path}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(a, orient='index')\n",
    "    df = df.transpose()\n",
    "    \n",
    "    df['xmin'] = -1\n",
    "    df['ymin'] = -1\n",
    "    df['xmax'] = -1\n",
    "    df['ymax'] = -1\n",
    "\n",
    "    df[['xmin','ymin','xmax','ymax']] = np.stack([df['boxes'][i] for i in range(len(df['boxes']))])\n",
    "\n",
    "    df.drop(columns=['boxes'], inplace=True)\n",
    "    df['xmin'] = df['xmin'].astype('float32')\n",
    "    df['ymin'] = df['ymin'].astype('float32')\n",
    "    df['xmax'] = df['xmax'].astype('float32')\n",
    "    df['ymax'] = df['ymax'].astype('float32')\n",
    "    \n",
    "    df['id'] = df['image_id'].map(lambda x: x.split(\".jpg\")[0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def build_df(xml_files):\n",
    "    # parse xml files and create pandas dataframe\n",
    "    df = xml_files_to_df(xml_files)\n",
    "\n",
    "    classes = sorted(df['names'].unique())\n",
    "    print(classes)\n",
    "    cls2id = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    df['label'] = df['names'].map(cls2id)\n",
    "    \n",
    "    # in this df, each object of a given image is in a separate row\n",
    "    df = df[['id', 'label', 'xmin', 'ymin', 'xmax', 'ymax', 'img_path']]\n",
    "    \n",
    "    return df, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:20.544708Z",
     "iopub.status.busy": "2022-08-19T12:52:20.542664Z",
     "iopub.status.idle": "2022-08-19T12:52:23.660911Z",
     "shell.execute_reply": "2022-08-19T12:52:23.659783Z",
     "shell.execute_reply.started": "2022-08-19T12:52:20.544659Z"
    },
    "id": "knCEq7-qLAZZ",
    "outputId": "6cd935ef-7918-49cd-e0e6-2c2eff1ce84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "df, classes = build_df(XML_FILES)\n",
    "cls2id = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "id2cls = {i: cls_name for i, cls_name in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "print(len(classes), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007_000027</td>\n",
       "      <td>14</td>\n",
       "      <td>174.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>14</td>\n",
       "      <td>195.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>14</td>\n",
       "      <td>26.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40133</th>\n",
       "      <td>2012_004328</td>\n",
       "      <td>14</td>\n",
       "      <td>59.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40134</th>\n",
       "      <td>2012_004328</td>\n",
       "      <td>14</td>\n",
       "      <td>219.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40135</th>\n",
       "      <td>2012_004329</td>\n",
       "      <td>14</td>\n",
       "      <td>57.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40136</th>\n",
       "      <td>2012_004330</td>\n",
       "      <td>14</td>\n",
       "      <td>230.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40137</th>\n",
       "      <td>2012_004331</td>\n",
       "      <td>14</td>\n",
       "      <td>102.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40138 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  label   xmin   ymin   xmax   ymax  \\\n",
       "0      2007_000027     14  174.0  101.0  349.0  351.0   \n",
       "1      2007_000032      0  104.0   78.0  375.0  183.0   \n",
       "2      2007_000032      0  133.0   88.0  197.0  123.0   \n",
       "3      2007_000032     14  195.0  180.0  213.0  229.0   \n",
       "4      2007_000032     14   26.0  189.0   44.0  238.0   \n",
       "...            ...    ...    ...    ...    ...    ...   \n",
       "40133  2012_004328     14   59.0  220.0  166.0  415.0   \n",
       "40134  2012_004328     14  219.0  226.0  268.0  332.0   \n",
       "40135  2012_004329     14   57.0   88.0  284.0  397.0   \n",
       "40136  2012_004330     14  230.0  133.0  370.0  441.0   \n",
       "40137  2012_004331     14  102.0   25.0  208.0  230.0   \n",
       "\n",
       "                                                img_path  \n",
       "0      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...  \n",
       "1      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...  \n",
       "2      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...  \n",
       "3      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...  \n",
       "4      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...  \n",
       "...                                                  ...  \n",
       "40133  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...  \n",
       "40134  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...  \n",
       "40135  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...  \n",
       "40136  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...  \n",
       "40137  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...  \n",
       "\n",
       "[40138 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMGS = 'datasets/17kGraffiti/split/train/graffiti/'\n",
    "# DataFrame,dictionary of lists, unpickled_data\n",
    "train_bboxes = pd.read_pickle('datasets/17kGraffiti/train_bboxes.pkl')\n",
    "train_bboxes[\"img_path\"] = train_bboxes[\"FileName\"].apply(lambda x: f\"{TRAIN_IMGS}{x}.jpg\")\n",
    "train_bboxes.rename(columns={\"FileName\": \"id\"}, inplace=True)\n",
    "train_bboxes = train_bboxes.explode('bbox').copy()\n",
    "train_bboxes[\"label\"] = 0\n",
    "train_bboxes[\"xmin\"] = train_bboxes[\"bbox\"].apply(lambda x: float(x[0]))\n",
    "train_bboxes[\"ymin\"] = train_bboxes[\"bbox\"].apply(lambda x: float(x[1]))\n",
    "train_bboxes[\"xmax\"] = train_bboxes[\"bbox\"].apply(lambda x: float(x[2]))\n",
    "train_bboxes[\"ymax\"] = train_bboxes[\"bbox\"].apply(lambda x: float(x[3]))\n",
    "train_bboxes.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bbox_count</th>\n",
       "      <th>bbox</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10005452375_1fc378fb7f_c</td>\n",
       "      <td>1</td>\n",
       "      <td>[103, 93, 572, 436]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10005549113_1057d20b58_c</td>\n",
       "      <td>1</td>\n",
       "      <td>[106, 30, 659, 533]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10007929605_a3e9d26cd7_c</td>\n",
       "      <td>1</td>\n",
       "      <td>[5, 286, 781, 645]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10008858965_1fe2600f5e_c</td>\n",
       "      <td>1</td>\n",
       "      <td>[34, 13, 770, 584]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10008863645_708e989a49_c</td>\n",
       "      <td>1</td>\n",
       "      <td>[7, 84, 781, 511]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13814</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>6</td>\n",
       "      <td>[390, 143, 514, 286]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13815</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>6</td>\n",
       "      <td>[553, 158, 648, 291]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13816</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>6</td>\n",
       "      <td>[678, 183, 733, 269]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13817</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>6</td>\n",
       "      <td>[465, 458, 731, 586]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13818</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>6</td>\n",
       "      <td>[74, 478, 260, 568]</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13819 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  bbox_count                  bbox  \\\n",
       "0      10005452375_1fc378fb7f_c           1   [103, 93, 572, 436]   \n",
       "1      10005549113_1057d20b58_c           1   [106, 30, 659, 533]   \n",
       "2      10007929605_a3e9d26cd7_c           1    [5, 286, 781, 645]   \n",
       "3      10008858965_1fe2600f5e_c           1    [34, 13, 770, 584]   \n",
       "4      10008863645_708e989a49_c           1     [7, 84, 781, 511]   \n",
       "...                         ...         ...                   ...   \n",
       "13814  16064943625_48c18d21d8_c           6  [390, 143, 514, 286]   \n",
       "13815  16064943625_48c18d21d8_c           6  [553, 158, 648, 291]   \n",
       "13816  16064943625_48c18d21d8_c           6  [678, 183, 733, 269]   \n",
       "13817  16064943625_48c18d21d8_c           6  [465, 458, 731, 586]   \n",
       "13818  16064943625_48c18d21d8_c           6   [74, 478, 260, 568]   \n",
       "\n",
       "                                                img_path  label   xmin   ymin  \\\n",
       "0      datasets/17kGraffiti/split/train/graffiti/1000...      0  103.0   93.0   \n",
       "1      datasets/17kGraffiti/split/train/graffiti/1000...      0  106.0   30.0   \n",
       "2      datasets/17kGraffiti/split/train/graffiti/1000...      0    5.0  286.0   \n",
       "3      datasets/17kGraffiti/split/train/graffiti/1000...      0   34.0   13.0   \n",
       "4      datasets/17kGraffiti/split/train/graffiti/1000...      0    7.0   84.0   \n",
       "...                                                  ...    ...    ...    ...   \n",
       "13814  datasets/17kGraffiti/split/train/graffiti/1606...      0  390.0  143.0   \n",
       "13815  datasets/17kGraffiti/split/train/graffiti/1606...      0  553.0  158.0   \n",
       "13816  datasets/17kGraffiti/split/train/graffiti/1606...      0  678.0  183.0   \n",
       "13817  datasets/17kGraffiti/split/train/graffiti/1606...      0  465.0  458.0   \n",
       "13818  datasets/17kGraffiti/split/train/graffiti/1606...      0   74.0  478.0   \n",
       "\n",
       "        xmax   ymax  \n",
       "0      572.0  436.0  \n",
       "1      659.0  533.0  \n",
       "2      781.0  645.0  \n",
       "3      770.0  584.0  \n",
       "4      781.0  511.0  \n",
       "...      ...    ...  \n",
       "13814  514.0  286.0  \n",
       "13815  648.0  291.0  \n",
       "13816  733.0  269.0  \n",
       "13817  731.0  586.0  \n",
       "13818  260.0  568.0  \n",
       "\n",
       "[13819 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMGS = 'datasets/17kGraffiti/split/test/graffiti/'\n",
    "test_bboxes = pd.read_pickle('datasets/17kGraffiti/test_bboxes.pkl')\n",
    "test_bboxes = test_bboxes.astype(object)\n",
    "test_bboxes[\"img_path\"] = test_bboxes[\"FileName\"].apply(lambda x: f\"{TEST_IMGS}{x}.jpg\")\n",
    "test_bboxes.rename(columns={\"FileName\": \"id\"}, inplace=True)\n",
    "test_bboxes = test_bboxes.explode('bbox').copy()\n",
    "test_bboxes[\"label\"] = 0\n",
    "test_bboxes[\"xmin\"] = test_bboxes[\"bbox\"].apply(lambda x: float(x[0]))\n",
    "test_bboxes[\"ymin\"] = test_bboxes[\"bbox\"].apply(lambda x: float(x[1]))\n",
    "test_bboxes[\"xmax\"] = test_bboxes[\"bbox\"].apply(lambda x: float(x[2]))\n",
    "test_bboxes[\"ymax\"] = test_bboxes[\"bbox\"].apply(lambda x: float(x[3]))\n",
    "test_bboxes.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bbox_count</th>\n",
       "      <th>bbox</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10005509946_302a7e4b81_c</td>\n",
       "      <td>2</td>\n",
       "      <td>[7, 108, 153, 377]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/10005...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10005509946_302a7e4b81_c</td>\n",
       "      <td>2</td>\n",
       "      <td>[186, 62, 716, 387]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/10005...</td>\n",
       "      <td>0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10008085844_aa0cb2ec39_c</td>\n",
       "      <td>1</td>\n",
       "      <td>[18, 39, 773, 241]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/10008...</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10034314145_18c252992b_c</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 93, 144, 303]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/10034...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10034314145_18c252992b_c</td>\n",
       "      <td>3</td>\n",
       "      <td>[85, 58, 259, 288]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/10034...</td>\n",
       "      <td>0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>29566606407_70d4dac576_c</td>\n",
       "      <td>5</td>\n",
       "      <td>[63, 170, 344, 290]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/29566...</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>29566606407_70d4dac576_c</td>\n",
       "      <td>5</td>\n",
       "      <td>[354, 174, 509, 277]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/29566...</td>\n",
       "      <td>0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>29566606407_70d4dac576_c</td>\n",
       "      <td>5</td>\n",
       "      <td>[505, 202, 606, 273]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/29566...</td>\n",
       "      <td>0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>29566606407_70d4dac576_c</td>\n",
       "      <td>5</td>\n",
       "      <td>[600, 208, 677, 279]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/29566...</td>\n",
       "      <td>0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>29566606407_70d4dac576_c</td>\n",
       "      <td>5</td>\n",
       "      <td>[676, 218, 786, 270]</td>\n",
       "      <td>datasets/17kGraffiti/split/test/graffiti/29566...</td>\n",
       "      <td>0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3012 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id bbox_count                  bbox  \\\n",
       "0     10005509946_302a7e4b81_c          2    [7, 108, 153, 377]   \n",
       "1     10005509946_302a7e4b81_c          2   [186, 62, 716, 387]   \n",
       "2     10008085844_aa0cb2ec39_c          1    [18, 39, 773, 241]   \n",
       "3     10034314145_18c252992b_c          3     [3, 93, 144, 303]   \n",
       "4     10034314145_18c252992b_c          3    [85, 58, 259, 288]   \n",
       "...                        ...        ...                   ...   \n",
       "3007  29566606407_70d4dac576_c          5   [63, 170, 344, 290]   \n",
       "3008  29566606407_70d4dac576_c          5  [354, 174, 509, 277]   \n",
       "3009  29566606407_70d4dac576_c          5  [505, 202, 606, 273]   \n",
       "3010  29566606407_70d4dac576_c          5  [600, 208, 677, 279]   \n",
       "3011  29566606407_70d4dac576_c          5  [676, 218, 786, 270]   \n",
       "\n",
       "                                               img_path  label   xmin   ymin  \\\n",
       "0     datasets/17kGraffiti/split/test/graffiti/10005...      0    7.0  108.0   \n",
       "1     datasets/17kGraffiti/split/test/graffiti/10005...      0  186.0   62.0   \n",
       "2     datasets/17kGraffiti/split/test/graffiti/10008...      0   18.0   39.0   \n",
       "3     datasets/17kGraffiti/split/test/graffiti/10034...      0    3.0   93.0   \n",
       "4     datasets/17kGraffiti/split/test/graffiti/10034...      0   85.0   58.0   \n",
       "...                                                 ...    ...    ...    ...   \n",
       "3007  datasets/17kGraffiti/split/test/graffiti/29566...      0   63.0  170.0   \n",
       "3008  datasets/17kGraffiti/split/test/graffiti/29566...      0  354.0  174.0   \n",
       "3009  datasets/17kGraffiti/split/test/graffiti/29566...      0  505.0  202.0   \n",
       "3010  datasets/17kGraffiti/split/test/graffiti/29566...      0  600.0  208.0   \n",
       "3011  datasets/17kGraffiti/split/test/graffiti/29566...      0  676.0  218.0   \n",
       "\n",
       "       xmax   ymax  \n",
       "0     153.0  377.0  \n",
       "1     716.0  387.0  \n",
       "2     773.0  241.0  \n",
       "3     144.0  303.0  \n",
       "4     259.0  288.0  \n",
       "...     ...    ...  \n",
       "3007  344.0  290.0  \n",
       "3008  509.0  277.0  \n",
       "3009  606.0  273.0  \n",
       "3010  677.0  279.0  \n",
       "3011  786.0  270.0  \n",
       "\n",
       "[3012 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzttaFsiLAZZ"
   },
   "source": [
    "## Split dataframe to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:23.663664Z",
     "iopub.status.busy": "2022-08-19T12:52:23.663254Z",
     "iopub.status.idle": "2022-08-19T12:52:23.671682Z",
     "shell.execute_reply": "2022-08-19T12:52:23.670330Z",
     "shell.execute_reply.started": "2022-08-19T12:52:23.663627Z"
    },
    "id": "Vr_CwHNlLAZZ"
   },
   "outputs": [],
   "source": [
    "def split_df(df, n_folds=5, training_fold=0):\n",
    "    mapping = df.groupby(\"id\")['img_path'].agg(len).to_dict()\n",
    "    df['stratify'] = df['id'].map(mapping)\n",
    "\n",
    "    kfold = StratifiedGroupKFold(\n",
    "        n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for i, (_, val_idx) in enumerate(kfold.split(df, y=df['stratify'], groups=df['id'])):\n",
    "        df.loc[val_idx, 'fold'] = i\n",
    "\n",
    "    train_df = df[df['fold'] != training_fold].reset_index(drop=True)\n",
    "    valid_df = df[df['fold'] == training_fold].reset_index(drop=True)\n",
    "\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  13700\n",
      "Valid size:  3425\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = split_df(df)\n",
    "print(\"Train size: \", train_df['id'].nunique())\n",
    "print(\"Valid size: \", valid_df['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>img_path</th>\n",
       "      <th>stratify</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007_000027</td>\n",
       "      <td>14</td>\n",
       "      <td>174.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>14</td>\n",
       "      <td>195.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007_000032</td>\n",
       "      <td>14</td>\n",
       "      <td>26.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32018</th>\n",
       "      <td>2012_004328</td>\n",
       "      <td>14</td>\n",
       "      <td>59.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32019</th>\n",
       "      <td>2012_004328</td>\n",
       "      <td>14</td>\n",
       "      <td>219.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32020</th>\n",
       "      <td>2012_004329</td>\n",
       "      <td>14</td>\n",
       "      <td>57.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32021</th>\n",
       "      <td>2012_004330</td>\n",
       "      <td>14</td>\n",
       "      <td>230.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32022</th>\n",
       "      <td>2012_004331</td>\n",
       "      <td>14</td>\n",
       "      <td>102.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32023 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  label   xmin   ymin   xmax   ymax  \\\n",
       "0      2007_000027     14  174.0  101.0  349.0  351.0   \n",
       "1      2007_000032      0  104.0   78.0  375.0  183.0   \n",
       "2      2007_000032      0  133.0   88.0  197.0  123.0   \n",
       "3      2007_000032     14  195.0  180.0  213.0  229.0   \n",
       "4      2007_000032     14   26.0  189.0   44.0  238.0   \n",
       "...            ...    ...    ...    ...    ...    ...   \n",
       "32018  2012_004328     14   59.0  220.0  166.0  415.0   \n",
       "32019  2012_004328     14  219.0  226.0  268.0  332.0   \n",
       "32020  2012_004329     14   57.0   88.0  284.0  397.0   \n",
       "32021  2012_004330     14  230.0  133.0  370.0  441.0   \n",
       "32022  2012_004331     14  102.0   25.0  208.0  230.0   \n",
       "\n",
       "                                                img_path  stratify  fold  \n",
       "0      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...         1   1.0  \n",
       "1      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...         4   1.0  \n",
       "2      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...         4   1.0  \n",
       "3      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...         4   1.0  \n",
       "4      datasets/VOCdevkit/VOC2012/JPEGImages/2007_000...         4   1.0  \n",
       "...                                                  ...       ...   ...  \n",
       "32018  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...         2   3.0  \n",
       "32019  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...         2   3.0  \n",
       "32020  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...         1   3.0  \n",
       "32021  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...         1   2.0  \n",
       "32022  datasets/VOCdevkit/VOC2012/JPEGImages/2012_004...         1   1.0  \n",
       "\n",
       "[32023 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:23.674592Z",
     "iopub.status.busy": "2022-08-19T12:52:23.673687Z",
     "iopub.status.idle": "2022-08-19T12:52:31.097148Z",
     "shell.execute_reply": "2022-08-19T12:52:31.096052Z",
     "shell.execute_reply.started": "2022-08-19T12:52:23.674553Z"
    },
    "id": "ukf32XNZLAZZ",
    "outputId": "23d6e7b6-1ebf-411d-fc2a-f6497a118a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  5559\n",
      "Valid size:  1390\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = split_df(train_bboxes.drop(columns=[\"bbox_count\", \"bbox\"]))\n",
    "print(\"Train size: \", train_df['id'].nunique())\n",
    "print(\"Valid size: \", valid_df['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>img_path</th>\n",
       "      <th>stratify</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007929605_a3e9d26cd7_c</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10008858965_1fe2600f5e_c</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10008863645_708e989a49_c</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10008916523_57e1215f5a_c</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10008971653_d32f09b87b_c</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11038</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11039</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11040</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11041</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11042</th>\n",
       "      <td>16064943625_48c18d21d8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1606...</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11043 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  label   xmin   ymin   xmax   ymax  \\\n",
       "0      10007929605_a3e9d26cd7_c      0    5.0  286.0  781.0  645.0   \n",
       "1      10008858965_1fe2600f5e_c      0   34.0   13.0  770.0  584.0   \n",
       "2      10008863645_708e989a49_c      0    7.0   84.0  781.0  511.0   \n",
       "3      10008916523_57e1215f5a_c      0   15.0   67.0  790.0  511.0   \n",
       "4      10008971653_d32f09b87b_c      0   66.0  105.0  757.0  505.0   \n",
       "...                         ...    ...    ...    ...    ...    ...   \n",
       "11038  16064943625_48c18d21d8_c      0  390.0  143.0  514.0  286.0   \n",
       "11039  16064943625_48c18d21d8_c      0  553.0  158.0  648.0  291.0   \n",
       "11040  16064943625_48c18d21d8_c      0  678.0  183.0  733.0  269.0   \n",
       "11041  16064943625_48c18d21d8_c      0  465.0  458.0  731.0  586.0   \n",
       "11042  16064943625_48c18d21d8_c      0   74.0  478.0  260.0  568.0   \n",
       "\n",
       "                                                img_path  stratify  fold  \n",
       "0      datasets/17kGraffiti/split/train/graffiti/1000...         1   3.0  \n",
       "1      datasets/17kGraffiti/split/train/graffiti/1000...         1   2.0  \n",
       "2      datasets/17kGraffiti/split/train/graffiti/1000...         1   4.0  \n",
       "3      datasets/17kGraffiti/split/train/graffiti/1000...         1   1.0  \n",
       "4      datasets/17kGraffiti/split/train/graffiti/1000...         1   3.0  \n",
       "...                                                  ...       ...   ...  \n",
       "11038  datasets/17kGraffiti/split/train/graffiti/1606...         6   1.0  \n",
       "11039  datasets/17kGraffiti/split/train/graffiti/1606...         6   1.0  \n",
       "11040  datasets/17kGraffiti/split/train/graffiti/1606...         6   1.0  \n",
       "11041  datasets/17kGraffiti/split/train/graffiti/1606...         6   1.0  \n",
       "11042  datasets/17kGraffiti/split/train/graffiti/1606...         6   1.0  \n",
       "\n",
       "[11043 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[['id', 'label', 'xmin', 'ymin', 'xmax', 'ymax', 'img_path', 'stratify', 'fold']].copy()\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>img_path</th>\n",
       "      <th>stratify</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10005452375_1fc378fb7f_c</td>\n",
       "      <td>0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>436.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10005549113_1057d20b58_c</td>\n",
       "      <td>0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10009017873_ac4b0ee3e7_c</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1000...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10034338656_423b52122d_c</td>\n",
       "      <td>0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>778.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1003...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10041596723_a97ea3fce3_c</td>\n",
       "      <td>0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/1004...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>30870264967_a5deb8cdf8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/3087...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>30870264967_a5deb8cdf8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/3087...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>30870264967_a5deb8cdf8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>656.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/3087...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>30870264967_a5deb8cdf8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/3087...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2775</th>\n",
       "      <td>30870264967_a5deb8cdf8_c</td>\n",
       "      <td>0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>datasets/17kGraffiti/split/train/graffiti/3087...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2776 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  label   xmin   ymin   xmax   ymax  \\\n",
       "0     10005452375_1fc378fb7f_c      0  103.0   93.0  572.0  436.0   \n",
       "1     10005549113_1057d20b58_c      0  106.0   30.0  659.0  533.0   \n",
       "2     10009017873_ac4b0ee3e7_c      0    9.0   83.0  757.0  498.0   \n",
       "3     10034338656_423b52122d_c      0   83.0   36.0  778.0  358.0   \n",
       "4     10041596723_a97ea3fce3_c      0  153.0  269.0  770.0  442.0   \n",
       "...                        ...    ...    ...    ...    ...    ...   \n",
       "2771  30870264967_a5deb8cdf8_c      0  384.0  241.0  709.0  338.0   \n",
       "2772  30870264967_a5deb8cdf8_c      0  490.0   94.0  698.0  154.0   \n",
       "2773  30870264967_a5deb8cdf8_c      0  656.0  150.0  786.0  226.0   \n",
       "2774  30870264967_a5deb8cdf8_c      0  724.0  101.0  786.0  156.0   \n",
       "2775  30870264967_a5deb8cdf8_c      0  705.0  232.0  786.0  323.0   \n",
       "\n",
       "                                               img_path  stratify  fold  \n",
       "0     datasets/17kGraffiti/split/train/graffiti/1000...         1   0.0  \n",
       "1     datasets/17kGraffiti/split/train/graffiti/1000...         1   0.0  \n",
       "2     datasets/17kGraffiti/split/train/graffiti/1000...         1   0.0  \n",
       "3     datasets/17kGraffiti/split/train/graffiti/1003...         1   0.0  \n",
       "4     datasets/17kGraffiti/split/train/graffiti/1004...         1   0.0  \n",
       "...                                                 ...       ...   ...  \n",
       "2771  datasets/17kGraffiti/split/train/graffiti/3087...        11   0.0  \n",
       "2772  datasets/17kGraffiti/split/train/graffiti/3087...        11   0.0  \n",
       "2773  datasets/17kGraffiti/split/train/graffiti/3087...        11   0.0  \n",
       "2774  datasets/17kGraffiti/split/train/graffiti/3087...        11   0.0  \n",
       "2775  datasets/17kGraffiti/split/train/graffiti/3087...        11   0.0  \n",
       "\n",
       "[2776 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = valid_df[['id', 'label', 'xmin', 'ymin', 'xmax', 'ymax', 'img_path', 'stratify', 'fold']].copy()\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYUu7nxkLAZa"
   },
   "source": [
    "# Building Dataset and Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pigtDEkh0KY"
   },
   "source": [
    "As I mentioned earlier, we will use VOC 2012 dataset with 17125 images and their corresponding objects from 20 classes. The paper uses COCO dataset which is an order of magnitude larger than VOC and they also pre-train the models on a much larger dataset before training on COCO. But, to stay simple, I'm gonna use this rather small VOC dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82M8w0bmiDIP"
   },
   "source": [
    "```classes = [\n",
    "  \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \n",
    "  \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \n",
    "  \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\"\n",
    "  \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.131663Z",
     "iopub.status.busy": "2022-08-19T12:52:31.131250Z",
     "iopub.status.idle": "2022-08-19T12:52:31.140581Z",
     "shell.execute_reply": "2022-08-19T12:52:31.139645Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.131593Z"
    },
    "id": "YGs0_w6QLAZa"
   },
   "outputs": [],
   "source": [
    "def get_transform_train(size):\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(),\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_transform_valid(size):\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(),\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi98per_iLVd"
   },
   "source": [
    "We need a PyTorch dataset class that gives us an image and its bbox coordinates and classes in form of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.142768Z",
     "iopub.status.busy": "2022-08-19T12:52:31.142099Z",
     "iopub.status.idle": "2022-08-19T12:52:31.153376Z",
     "shell.execute_reply": "2022-08-19T12:52:31.152432Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.142732Z"
    },
    "id": "5VhcKax-LAZa"
   },
   "outputs": [],
   "source": [
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transforms=None, tokenizer=None):\n",
    "        self.ids = df['id'].unique()\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df[self.df['id'] == self.ids[idx]]\n",
    "        img_path = sample['img_path'].values[0]\n",
    "\n",
    "        img = cv2.imread(img_path)[..., ::-1]\n",
    "        labels = sample['label'].values\n",
    "        bboxes = sample[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(**{\n",
    "                'image': img,\n",
    "                'bboxes': bboxes,\n",
    "                'labels': labels\n",
    "            })\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "\n",
    "        img = torch.FloatTensor(img).permute(2, 0, 1)\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            seqs = self.tokenizer(labels, bboxes)\n",
    "            seqs = torch.LongTensor(seqs)\n",
    "            return img, seqs\n",
    "\n",
    "        return img, labels, bboxes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9yBtAZTiOHV"
   },
   "source": [
    "As you see, most of the code here is what you expect from a simple dataset for classification but there are small differences too. We need a Tokenizer to convert our labels and bbox coordinates (x and y) to a sequence so that we can perform train our model for the language modeling task (predicting the next tokens conditioned on the previously seen tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18UuPNZrLAZZ"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2N4SgMEria4i"
   },
   "source": [
    "How are we going to convert these information into a sequence? Well, it's not that difficult. To represent an object in an image, we need 5 numbers: 4 coordinate numbers and 1 to indicate which class it belongs to. \n",
    "You actually need to know the coordinates of 2 points of a bounding box to be able to draw it in an image; in pascal format, we use the top left point and the bottom right point of the bbox as those 2 critical points and each point is represented by its x and y values → so, we will need 4 numbers overall to draw a bounding box. You can see alternative formats to represent a bounding box down below. Also, look at where the start of x and y axis is (the 0, 0 point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouQwaMdigI9"
   },
   "source": [
    "![](https://albumentations.ai/docs/images/getting_started/augmenting_bboxes/bbox_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DUCg8-4ipig"
   },
   "source": [
    "As you see in the dataset's code, we give the bbox coordinates and labels to our tokenizer and get a simple list of tokens out. The tokenizer needs to do the following tasks:\n",
    "1. mark the start and end of the sequence w/ especial tokens (BOS and EOS tokens).\n",
    "2. quantize the continuous value of coordinates (we can have x=34.7 as the coordinate of a point but we need discrete values like 34 as our tokens because we are finally doing a classification on a finite set of tokens)\n",
    "3. encode the label of the objects into their corresponding tokens\n",
    "4. randomize the order of objects in the final sequence (more on this below)\n",
    "\n",
    "If you are familiar with NLP applications, these steps might sound familiar to you as they are also done when we are dealing with words in a natural language; we need to tokenize them and assign each word to its own discrete token, mark the start and end of the sequence, etc.\n",
    "Regarding the number 4 in this list, this is what the paper does and there is an extensive ablation study on whether it is a good idea. What it says is that each time that we show the same image to the model (in different epochs), we randomize the order in which the objects appear in the corresponding sequence which we feed to the model (with one token shifted) and our loss function. For example, if there is a \"person\", a \"car\", and a \"cat\" in an image, the tokenizer and dataset will put these objects in random order in the sequence: \n",
    "- BOS, car_xmin, car_ymin, car_xmax, car_ymax, car_label, person_xmin, person_ymin, person_xmax, person_ymax, person_label, cat_xmin, cat_ymin, cat_xmax, cat_ymax, cat_label, EOS\n",
    "- BOS, person_xmin, person_ymin, person_xmax, person_ymax, person_label, car_xmin, car_ymin, car_xmax, car_ymax, car_label, cat_xmin, cat_ymin, cat_xmax, cat_ymax, cat_label, EOS\n",
    "- …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7ChjEy9uiUuw"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, num_classes: int, num_bins: int, width: int, height: int, max_len=500):\n",
    "        self.num_classes = num_classes\n",
    "        self.num_bins = num_bins\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.BOS_code = num_classes + num_bins\n",
    "        self.EOS_code = self.BOS_code + 1\n",
    "        self.PAD_code = self.EOS_code + 1\n",
    "\n",
    "        self.vocab_size = num_classes + num_bins + 3\n",
    "\n",
    "    def quantize(self, x: np.array):\n",
    "        \"\"\"\n",
    "        x is a real number in [0, 1]\n",
    "        \"\"\"\n",
    "        return (x * (self.num_bins - 1)).astype('int')\n",
    "    \n",
    "    def dequantize(self, x: np.array):\n",
    "        \"\"\"\n",
    "        x is an integer between [0, num_bins-1]\n",
    "        \"\"\"\n",
    "        return x.astype('float32') / (self.num_bins - 1)\n",
    "\n",
    "    def __call__(self, labels: list, bboxes: list, shuffle=True):\n",
    "        assert len(labels) == len(bboxes), \"labels and bboxes must have the same length\"\n",
    "        bboxes = np.array(bboxes)\n",
    "        labels = np.array(labels)\n",
    "        labels += self.num_bins\n",
    "        labels = labels.astype('int')[:self.max_len]\n",
    "\n",
    "        bboxes[:, 0] = bboxes[:, 0] / self.width\n",
    "        bboxes[:, 2] = bboxes[:, 2] / self.width\n",
    "        bboxes[:, 1] = bboxes[:, 1] / self.height\n",
    "        bboxes[:, 3] = bboxes[:, 3] / self.height\n",
    "\n",
    "        bboxes = self.quantize(bboxes)[:self.max_len]\n",
    "\n",
    "        if shuffle:\n",
    "            rand_idxs = np.arange(0, len(bboxes))\n",
    "            np.random.shuffle(rand_idxs)\n",
    "            labels = labels[rand_idxs]\n",
    "            bboxes = bboxes[rand_idxs]\n",
    "\n",
    "        tokenized = [self.BOS_code]\n",
    "        for label, bbox in zip(labels, bboxes):\n",
    "            tokens = list(bbox)\n",
    "            tokens.append(label)\n",
    "\n",
    "            tokenized.extend(list(map(int, tokens)))\n",
    "        tokenized.append(self.EOS_code)\n",
    "\n",
    "        return tokenized    \n",
    "    \n",
    "    def decode(self, tokens: torch.tensor):\n",
    "        \"\"\"\n",
    "        toekns: torch.LongTensor with shape [L]\n",
    "        \"\"\"\n",
    "        mask = tokens != self.PAD_code\n",
    "        tokens = tokens[mask]\n",
    "        tokens = tokens[1:-1]\n",
    "        assert len(tokens) % 5 == 0, \"invalid tokens\"\n",
    "\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        for i in range(4, len(tokens)+1, 5):\n",
    "            label = tokens[i]\n",
    "            bbox = tokens[i-4: i]\n",
    "            labels.append(int(label))\n",
    "            bboxes.append([int(item) for item in bbox])\n",
    "        labels = np.array(labels) - self.num_bins\n",
    "        bboxes = np.array(bboxes)\n",
    "        bboxes = self.dequantize(bboxes)\n",
    "        \n",
    "        bboxes[:, 0] = bboxes[:, 0] * self.width\n",
    "        bboxes[:, 2] = bboxes[:, 2] * self.width\n",
    "        bboxes[:, 1] = bboxes[:, 1] * self.height\n",
    "        bboxes[:, 3] = bboxes[:, 3] * self.height\n",
    "        \n",
    "        return labels, bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imieBODQixih"
   },
   "source": [
    "Another note on how to quantize the continuous values of coordinates: imagine that the image size is 224. You can have a bbox with these 4 coordinates (12.2, 35.8, 68.1, 120.5). \n",
    "You will need at least 224 tokens (num_bins) to be able to tokenize (quantize) these 4 numbers with a precision of 1 pixel (you will lose information below 1 pixel). As you see in the tokenizer code, to convert this bbox coordinates to their tokenized version, we need to do the following:\n",
    "1. normalize the coordinates (make them between 0 and 1 by dividing them by the max value = 224)\n",
    "2. do this: ```int(x * (num_bins-1))```\n",
    "\n",
    "so, the converted version will be: (12, 35, 67, 119). Remember that int() function in Python does not round the number to the closest integer, but it will keep only the integer part of the number. As you see, we have lost some information on the exact position of the bbox but it is still a very good approximation. We can use a larger number of tokens (num of bins, as stated in the paper) and we will have a more precise location. Our tokenizer also has decode() function which we will use to convert sequences into bbox coordinates and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"graffiti\"]\n",
    "cls2id = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "id2cls = {i: cls_name for i, cls_name in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0znRsw2ViYTp"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_classes=len(classes), num_bins=CFG.num_bins,\n",
    "                          width=CFG.img_size, height=CFG.img_size, max_len=CFG.max_len)\n",
    "CFG.pad_idx = tokenizer.PAD_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmuknxK4i2FZ"
   },
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB8Uze6Ji3wb"
   },
   "source": [
    "Here, we will implement a custom collate_function to give to our PyTorch data loader. This function will take care of Padding for us: to make all the sequences the same length by adding PAD_IDX to the shorter ones in order to be able to build a batch with them. We are going to pad the sequence to a fixed max length of 300 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.156241Z",
     "iopub.status.busy": "2022-08-19T12:52:31.155978Z",
     "iopub.status.idle": "2022-08-19T12:52:31.167066Z",
     "shell.execute_reply": "2022-08-19T12:52:31.166185Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.156216Z"
    },
    "id": "axsgqzt5LAZa"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, max_len, pad_idx):\n",
    "    \"\"\"\n",
    "    if max_len:\n",
    "        the sequences will all be padded to that length\n",
    "    \"\"\"\n",
    "    image_batch, seq_batch = [], []\n",
    "    for image, seq in batch:\n",
    "        image_batch.append(image)\n",
    "        seq_batch.append(seq)\n",
    "\n",
    "    seq_batch = pad_sequence(\n",
    "        seq_batch, padding_value=pad_idx, batch_first=True)\n",
    "    if max_len:\n",
    "        pad = torch.ones(seq_batch.size(0), max_len -\n",
    "                         seq_batch.size(1)).fill_(pad_idx).long()\n",
    "        seq_batch = torch.cat([seq_batch, pad], dim=1)\n",
    "    image_batch = torch.stack(image_batch)\n",
    "    return image_batch, seq_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.169753Z",
     "iopub.status.busy": "2022-08-19T12:52:31.168461Z",
     "iopub.status.idle": "2022-08-19T12:52:31.178695Z",
     "shell.execute_reply": "2022-08-19T12:52:31.177505Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.169713Z"
    },
    "id": "vSXffZDnLAZb"
   },
   "outputs": [],
   "source": [
    "def get_loaders(train_df, valid_df, tokenizer, img_size, batch_size, max_len, pad_idx, num_workers=2):\n",
    "\n",
    "    train_ds = VOCDataset(train_df, transforms=get_transform_train(\n",
    "        img_size), tokenizer=tokenizer)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_fn, max_len=max_len, pad_idx=pad_idx),\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    valid_ds = VOCDataset(valid_df, transforms=get_transform_valid(\n",
    "        img_size), tokenizer=tokenizer)\n",
    "\n",
    "    validloader = torch.utils.data.DataLoader(\n",
    "        valid_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=partial(collate_fn, max_len=max_len, pad_idx=pad_idx),\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return trainloader, validloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.183405Z",
     "iopub.status.busy": "2022-08-19T12:52:31.182418Z",
     "iopub.status.idle": "2022-08-19T12:52:31.198233Z",
     "shell.execute_reply": "2022-08-19T12:52:31.197169Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.183354Z"
    },
    "id": "M7w3-rKkLAZb"
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(\n",
    "        train_df, valid_df, tokenizer, CFG.img_size, CFG.batch_size, CFG.max_len, tokenizer.PAD_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auMe6DvZLAZb"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUoZcTz6jB2y"
   },
   "source": [
    "Finally arrived to the coolest part for every deep learning lover: The Model 😍\n",
    "Let's take a second look at the first image of this tutorial. First, we will need an encoder to take the input image and give us some embeddings (representations). The paper uses a ResNet50 (and also in other experiments uses ViT) but I decided to use DeiT. As the name suggests, this is a data efficient vision transformer and I thought it would be a good fit for our small dataset. Like ViT, it splits the image into patches and processes them like words in a sentence which again could be great for our task, as we will have a separate embedding for each of these patches and we can give them to our decoder in the next section to predict the target sequence (see it like translation from English to French, where our image is like a sentence in English and our target sequence containing the coordinates and labels of bboxes is like the equivalent sentence in French).\n",
    "I will use timm library to implement a pre-trained DeiT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.201095Z",
     "iopub.status.busy": "2022-08-19T12:52:31.200169Z",
     "iopub.status.idle": "2022-08-19T12:52:31.208805Z",
     "shell.execute_reply": "2022-08-19T12:52:31.207824Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.201054Z"
    },
    "id": "vtG_mZPkLAZb"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='deit3_small_patch16_384_in21ft1k', pretrained=False, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, num_classes=0, global_pool='', pretrained=pretrained)\n",
    "        self.bottleneck = nn.AdaptiveAvgPool1d(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        return self.bottleneck(features[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWleD_hljF6J"
   },
   "source": [
    "The bottleneck layer is to reduce the number of features of these embeddings to that of the decoder. The paper used a decoder dim of 256 and that's the reason why I am reducing it here using Average Pooling. Also, the first token in this model relates to the CLS token and I am skipping it in the forward method (```features[:, 1:]```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.210946Z",
     "iopub.status.busy": "2022-08-19T12:52:31.210492Z",
     "iopub.status.idle": "2022-08-19T12:52:31.227994Z",
     "shell.execute_reply": "2022-08-19T12:52:31.226888Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.210909Z"
    },
    "id": "KAXKjppQLAZb"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_length, dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.randn(1, CFG.max_len-1, dim) * .02)\n",
    "        self.decoder_pos_drop = nn.Dropout(p=0.05)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim, nhead=num_heads)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "        \n",
    "        self.encoder_pos_embed = nn.Parameter(torch.randn(1, encoder_length, dim) * .02)\n",
    "        self.encoder_pos_drop = nn.Dropout(p=0.05)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'encoder_pos_embed' in name or 'decoder_pos_embed' in name: \n",
    "                print(\"skipping pos_embed...\")\n",
    "                continue\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "        trunc_normal_(self.encoder_pos_embed, std=.02)\n",
    "        trunc_normal_(self.decoder_pos_embed, std=.02)\n",
    "        \n",
    "    \n",
    "    def forward(self, encoder_out, tgt):\n",
    "        \"\"\"\n",
    "        encoder_out: shape(N, L, D)\n",
    "        tgt: shape(N, L)\n",
    "        \"\"\"\n",
    "        \n",
    "        tgt_mask, tgt_padding_mask = create_mask(tgt)\n",
    "        tgt_embedding = self.embedding(tgt)\n",
    "        tgt_embedding = self.decoder_pos_drop(\n",
    "            tgt_embedding + self.decoder_pos_embed\n",
    "        )\n",
    "        \n",
    "        encoder_out = self.encoder_pos_drop(\n",
    "            encoder_out + self.encoder_pos_embed\n",
    "        )\n",
    "        \n",
    "        encoder_out = encoder_out.transpose(0, 1)\n",
    "        tgt_embedding = tgt_embedding.transpose(0, 1)\n",
    "        \n",
    "        preds = self.decoder(memory=encoder_out, \n",
    "                             tgt=tgt_embedding,\n",
    "                             tgt_mask=tgt_mask, \n",
    "                             tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        preds = preds.transpose(0, 1)\n",
    "        return self.output(preds)\n",
    "    \n",
    "    def predict(self, encoder_out, tgt):\n",
    "        length = tgt.size(1)\n",
    "        padding = torch.ones(tgt.size(0), CFG.max_len-length-1).fill_(CFG.pad_idx).long().to(tgt.device)\n",
    "        tgt = torch.cat([tgt, padding], dim=1)\n",
    "        tgt_mask, tgt_padding_mask = create_mask(tgt)\n",
    "        # is it necessary to multiply it by math.sqrt(d) ?\n",
    "        tgt_embedding = self.embedding(tgt)\n",
    "        tgt_embedding = self.decoder_pos_drop(\n",
    "            tgt_embedding + self.decoder_pos_embed\n",
    "        )\n",
    "        \n",
    "        encoder_out = self.encoder_pos_drop(\n",
    "            encoder_out + self.encoder_pos_embed\n",
    "        )\n",
    "        \n",
    "        encoder_out = encoder_out.transpose(0, 1)\n",
    "        tgt_embedding = tgt_embedding.transpose(0, 1)\n",
    "        \n",
    "        preds = self.decoder(memory=encoder_out, \n",
    "                             tgt=tgt_embedding,\n",
    "                             tgt_mask=tgt_mask, \n",
    "                             tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        preds = preds.transpose(0, 1)\n",
    "        return self.output(preds)[:, length-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tumUJbyYjLEE"
   },
   "source": [
    "Our decoder takes the patch embeddings of the input image and learns to predict the sequence containing bboxes. Here I am using PyTorch nn.TransformerDecoder module to implement a 6 layer decoder with a feature dimension of 256. We also need to add positional embeddings to the embeddings so that the model knows about each token's position in the sequence (I am adding positional embedding for both encoder tokens and decoder tokens. While we have to do this for the decoder, we might not need to add them to the encoder tokens as the DeiT model knows about the order of patches itself). I am doing this by those nn.Parameter modules which will learn 1 parameter per token position. Finally, we will use a nn.Linear layer to predict the next token from our vocabulary.\n",
    "The ```create_mask()``` function (you will see its definition in tge next section named **Utils**) gives us two masks needed for training the decoder: one to tell the model to ignore the PAD tokens and do not incorporate them in its attention modules and another to mask the future tokens in order to make the decoder predict tokens only by looking at the current token and the previous ones.\n",
    "\n",
    "The decoder's predict method takes the previously generated tokens, pads them to the max_length and predicts the next token for each sequence in the batch and returns those new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.230131Z",
     "iopub.status.busy": "2022-08-19T12:52:31.229475Z",
     "iopub.status.idle": "2022-08-19T12:52:31.241518Z",
     "shell.execute_reply": "2022-08-19T12:52:31.240510Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.230096Z"
    },
    "id": "oPyI62UmLAZb"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, image, tgt):\n",
    "        encoder_out = self.encoder(image)\n",
    "        preds = self.decoder(encoder_out, tgt)\n",
    "        return preds\n",
    "    def predict(self, image, tgt):\n",
    "        encoder_out = self.encoder(image)\n",
    "        preds = self.decoder.predict(encoder_out, tgt)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2RyrSCbjnEi"
   },
   "source": [
    "This is a simple class encapsulating the encoder and decoder. It also has a predict function which calls the predict function of Decoder to detect objects in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:31.243504Z",
     "iopub.status.busy": "2022-08-19T12:52:31.242965Z",
     "iopub.status.idle": "2022-08-19T12:52:33.933257Z",
     "shell.execute_reply": "2022-08-19T12:52:33.932029Z",
     "shell.execute_reply.started": "2022-08-19T12:52:31.243422Z"
    },
    "id": "l7V3wTghLAZb",
    "outputId": "bf69f684-5b48-4dde-de3a-f9c0df058ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping pos_embed...\n",
      "skipping pos_embed...\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(model_name=CFG.model_name, pretrained=True, out_dim=256)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size,\n",
    "                  encoder_length=CFG.num_patches, dim=256, num_heads=8, num_layers=6)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "model.to(CFG.device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G58GUpn9LAZb"
   },
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twZktlyDj286"
   },
   "source": [
    "Now let's see how we can train this model. Most of the following code is just standard PyTorch training boilerplate but there is a simple but important point in it. As mentioned earlier, we train the model like a language model (GPT for e.g.) and it works like this → the model needs to predict the next token only seeing the previous ones (tokens to the left). At the start, it only sees the BOS sentence and it needs to predict the next token, and so on and so forth. And this is achieved simply by this part:\n",
    "1. ```y_input = y[:, :-1]```\n",
    "2. ```y_expected = y[:, 1:]```\n",
    "3. ```preds = model(x, y_input)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:53:04.424268Z",
     "iopub.status.busy": "2022-08-19T12:53:04.423908Z",
     "iopub.status.idle": "2022-08-19T12:53:04.434287Z",
     "shell.execute_reply": "2022-08-19T12:53:04.433084Z",
     "shell.execute_reply.started": "2022-08-19T12:53:04.424238Z"
    },
    "id": "kiRQARfLLAZb"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, criterion, logger=None):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    \n",
    "    for x, y in tqdm_object:\n",
    "        x, y = x.to(CFG.device, non_blocking=True), y.to(CFG.device, non_blocking=True)\n",
    "        \n",
    "        y_input = y[:, :-1]\n",
    "        y_expected = y[:, 1:]\n",
    "        \n",
    "\n",
    "        preds = model(x, y_input)\n",
    "        loss = criterion(preds.reshape(-1, preds.shape[-1]), y_expected.reshape(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        loss_meter.update(loss.item(), x.size(0))\n",
    "        \n",
    "        lr = get_lr(optimizer)\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=f\"{lr:.6f}\")\n",
    "        if logger is not None:\n",
    "            logger.log({\"train_step_loss\": loss_meter.avg, 'lr': lr})\n",
    "    \n",
    "    return loss_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:33.950406Z",
     "iopub.status.busy": "2022-08-19T12:52:33.949962Z",
     "iopub.status.idle": "2022-08-19T12:52:33.960776Z",
     "shell.execute_reply": "2022-08-19T12:52:33.959774Z",
     "shell.execute_reply.started": "2022-08-19T12:52:33.950353Z"
    },
    "id": "7rNklWtcLAZc"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm_object:\n",
    "            x, y = x.to(CFG.device, non_blocking=True), y.to(CFG.device, non_blocking=True)\n",
    "\n",
    "            y_input = y[:, :-1]\n",
    "            y_expected = y[:, 1:]\n",
    "\n",
    "            preds = model(x, y_input)\n",
    "            loss = criterion(preds.reshape(-1, preds.shape[-1]), y_expected.reshape(-1))\n",
    "\n",
    "\n",
    "            loss_meter.update(loss.item(), x.size(0))\n",
    "    \n",
    "    return loss_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:33.963664Z",
     "iopub.status.busy": "2022-08-19T12:52:33.962028Z",
     "iopub.status.idle": "2022-08-19T12:52:33.973428Z",
     "shell.execute_reply": "2022-08-19T12:52:33.972360Z",
     "shell.execute_reply.started": "2022-08-19T12:52:33.963629Z"
    },
    "id": "_mHsFwbxLAZc"
   },
   "outputs": [],
   "source": [
    "def train_eval(model, \n",
    "               train_loader,\n",
    "               valid_loader,\n",
    "               criterion, \n",
    "               optimizer, \n",
    "               lr_scheduler,\n",
    "               step,\n",
    "               logger):\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        if logger is not None:\n",
    "            logger.log({\"Epoch\": epoch + 1})\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, \n",
    "                                 lr_scheduler if step == 'batch' else None, \n",
    "                                 criterion, logger=logger)\n",
    "        \n",
    "        valid_loss = valid_epoch(model, valid_loader, criterion)\n",
    "        print(f\"Valid loss: {valid_loss:.3f}\")\n",
    "        \n",
    "        if step == 'epoch':\n",
    "            pass\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_valid_loss.pth')\n",
    "            print(\"Saved Best Model\")\n",
    "        \n",
    "        if logger is not None:\n",
    "            logger.log({\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss\n",
    "            })\n",
    "            logger.save('best_valid_loss.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A2oeuLELAZc"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:52:33.975704Z",
     "iopub.status.busy": "2022-08-19T12:52:33.974951Z",
     "iopub.status.idle": "2022-08-19T12:52:33.987057Z",
     "shell.execute_reply": "2022-08-19T12:52:33.986122Z",
     "shell.execute_reply.started": "2022-08-19T12:52:33.975668Z"
    },
    "id": "0yZ3nsZ6LAZc"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=CFG.device))\n",
    "            == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float(\n",
    "        '-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(tgt):\n",
    "    \"\"\"\n",
    "    tgt: shape(N, L)\n",
    "    \"\"\"\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    tgt_padding_mask = (tgt == CFG.pad_idx)\n",
    "\n",
    "    return tgt_mask, tgt_padding_mask\n",
    "\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0]*3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "execution": {
     "iopub.execute_input": "2022-08-19T12:53:07.519005Z",
     "iopub.status.busy": "2022-08-19T12:53:07.518254Z",
     "iopub.status.idle": "2022-08-19T12:53:49.327238Z",
     "shell.execute_reply": "2022-08-19T12:53:49.324697Z",
     "shell.execute_reply.started": "2022-08-19T12:53:07.518955Z"
    },
    "id": "Ac4sVGfRLAZc",
    "outputId": "1e862311-40bb-4cca-c15c-debbe540001f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                   | 0/348 [00:00<?, ?it/s]/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.78it/s, lr=0.000095, train_loss=4.65]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 4.126\n",
      "Saved Best Model\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.77it/s, lr=0.000084, train_loss=4.07]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.925\n",
      "Saved Best Model\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.79it/s, lr=0.000074, train_loss=3.94]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.844\n",
      "Saved Best Model\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.78it/s, lr=0.000063, train_loss=3.85]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.803\n",
      "Saved Best Model\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.80it/s, lr=0.000052, train_loss=3.78]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.778\n",
      "Saved Best Model\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.78it/s, lr=0.000042, train_loss=3.71]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.754\n",
      "Saved Best Model\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.78it/s, lr=0.000031, train_loss=3.64]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.746\n",
      "Saved Best Model\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.79it/s, lr=0.000021, train_loss=3.56]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.765\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.79it/s, lr=0.000010, train_loss=3.47]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.794\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 348/348 [01:12<00:00,  4.78it/s, lr=0.000000, train_loss=3.4]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:06<00:00, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "num_training_steps = CFG.epochs * (len(train_loader.dataset) // CFG.batch_size)\n",
    "num_warmup_steps = int(0.05 * num_training_steps)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                               num_training_steps=num_training_steps,\n",
    "                                               num_warmup_steps=num_warmup_steps)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CFG.pad_idx)\n",
    "\n",
    "train_eval(model,\n",
    "           train_loader,\n",
    "           valid_loader,\n",
    "           criterion,\n",
    "           optimizer,\n",
    "           lr_scheduler=lr_scheduler,\n",
    "           step='batch',\n",
    "           logger=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBE2bGFfkXvi"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLz96_vjkZ-6"
   },
   "source": [
    "Now let's take a look at how we can generate a detection sequence with this model for a test image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB4tDguqk_oZ"
   },
   "source": [
    "The following generate() function shows the whole sequence generation pipeline → First, we will create a batch with shape (batch_size, 1) containing only a BOS token for each image in the batch. The model takes the images and these BOS tokens and then predicts the next token for each image. We take the model's predictions, perform softmax and argmax on it to get the predicted token and concatenate this newly predicted token with the previous batch_preds tensor which had BOS tokens. We then repeat this loop for max_len number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T12:53:59.132026Z",
     "iopub.status.busy": "2022-08-19T12:53:59.131310Z",
     "iopub.status.idle": "2022-08-19T12:54:01.544531Z",
     "shell.execute_reply": "2022-08-19T12:54:01.543161Z",
     "shell.execute_reply.started": "2022-08-19T12:53:59.131988Z"
    },
    "id": "oN2pL_6SLAZc"
   },
   "outputs": [],
   "source": [
    "def generate(model, x, tokenizer, max_len=50, top_k=0, top_p=1):\n",
    "    x = x.to(CFG.device)\n",
    "    batch_preds = torch.ones(x.size(0), 1).fill_(tokenizer.BOS_code).long().to(CFG.device)\n",
    "    confs = []\n",
    "    \n",
    "    if top_k != 0 or top_p != 1:\n",
    "        sample = lambda preds: torch.softmax(preds, dim=-1).multinomial(num_samples=1).view(-1, 1)\n",
    "    else:\n",
    "        sample = lambda preds: torch.softmax(preds, dim=-1).argmax(dim=-1).view(-1, 1)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len):\n",
    "            preds = model.predict(x, batch_preds)\n",
    "            ## If top_k and top_p are set to default, the following line does nothing!\n",
    "            preds = top_k_top_p_filtering(preds, top_k=top_k, top_p=top_p)\n",
    "            if i % 4 == 0:\n",
    "                confs_ = torch.softmax(preds, dim=-1).sort(axis=-1, descending=True)[0][:, 0].cpu()\n",
    "                confs.append(confs_)\n",
    "            preds = sample(preds)\n",
    "            batch_preds = torch.cat([batch_preds, preds], dim=1)\n",
    "    \n",
    "    return batch_preds.cpu(), confs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh_PCfuWleg7"
   },
   "source": [
    "We will also use this postprocess function to decode the predictions and get bbox coordinates and labels for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SanfvWWpleTa"
   },
   "outputs": [],
   "source": [
    "def postprocess(batch_preds, batch_confs, tokenizer):\n",
    "    EOS_idxs = (batch_preds == tokenizer.EOS_code).float().argmax(dim=-1)\n",
    "    ## sanity check\n",
    "    invalid_idxs = ((EOS_idxs - 1) % 5 != 0).nonzero().view(-1)\n",
    "    EOS_idxs[invalid_idxs] = 0\n",
    "    \n",
    "    all_bboxes = []\n",
    "    all_labels = []\n",
    "    all_confs = []\n",
    "    for i, EOS_idx in enumerate(EOS_idxs.tolist()):\n",
    "        if EOS_idx == 0:\n",
    "            all_bboxes.append(None)\n",
    "            all_labels.append(None)\n",
    "            all_confs.append(None)\n",
    "            continue\n",
    "        labels, bboxes = tokenizer.decode(batch_preds[i, :EOS_idx+1])\n",
    "        confs = [round(batch_confs[j][i].item(), 3) for j in range(len(bboxes))]\n",
    "        \n",
    "        all_bboxes.append(bboxes)\n",
    "        all_labels.append(labels)\n",
    "        all_confs.append(confs)\n",
    "        \n",
    "    return all_bboxes, all_labels, all_confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIOi_Lifl6OY",
    "outputId": "55642202-d59d-41e3-cad5-e0c8fa3cfa0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: linha 1: gdown: comando não encontrado\r\n"
     ]
    }
   ],
   "source": [
    "!gdown --id \"1qB8gmzCMq29DQbj7zhKPQ2aphGnisHkS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "us5m6xXklrFb",
    "outputId": "011f7ef0-2ea5-4d2d-ad32-7a2f78787da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping pos_embed...\n",
      "skipping pos_embed...\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(model_name=CFG.model_name, pretrained=False, out_dim=256)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size,\n",
    "                encoder_length=CFG.num_patches, dim=256, num_heads=8, num_layers=6)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "model.to(CFG.device)\n",
    "\n",
    "msg = model.load_state_dict(torch.load('./best_valid_loss.pth', map_location=CFG.device))\n",
    "print(msg)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ZlOlfutzmDHX"
   },
   "outputs": [],
   "source": [
    "img_paths = \"\"\"2007_002648.jpg 2009_001611.jpg 2009_001643.jpg 2009_003956.jpg 2010_001669.jpg 2011_005895.jpg 2012_001185.jpg 2008_004301.jpg 2009_001614.jpg 2009_001673.jpg 2010_000803.jpg 2011_001054.jpg 2011_006197.jpg 2012_002955.jpg 2009_001466.jpg 2009_001623.jpg 2009_003233.jpg 2010_001109.jpg 2011_001526.jpg 2011_006707.jpg 2012_003463.jpg\"\"\"\n",
    "img_paths = [\"datasets/VOCdevkit/VOC2012/JPEGImages/\" + path for path in img_paths.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = test_bboxes[\"img_path\"].values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yTO8uLtOn7mL"
   },
   "outputs": [],
   "source": [
    "class VOCDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, size):\n",
    "        self.img_paths = img_paths\n",
    "        self.transforms = A.Compose([A.Resize(size, size), A.Normalize()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "\n",
    "        img = cv2.imread(img_path)[..., ::-1]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)['image']\n",
    "\n",
    "        img = torch.FloatTensor(img).permute(2, 0, 1)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "aVBqA8yYn39D"
   },
   "outputs": [],
   "source": [
    "test_dataset = VOCDatasetTest(img_paths, size=CFG.img_size)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=len(img_paths), shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "N9r-kGBZoEbX"
   },
   "outputs": [],
   "source": [
    "GT_COLOR = (0, 255, 0) # Green\n",
    "PRED_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color, thickness=1):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    bbox = [int(item) for item in bbox]\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "   \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_min + text_width, y_min + int(text_height * 1.3)), color, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min+ int(text_height * 1.3)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name, color=PRED_COLOR, show=True):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name, color)\n",
    "    if show:\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YjngYGmnuvo",
    "outputId": "6682b542-d0eb-4266-b879-af109e479308"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.35s/it]\n"
     ]
    }
   ],
   "source": [
    "all_bboxes = []\n",
    "all_labels = []\n",
    "all_confs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(test_loader):\n",
    "        batch_preds, batch_confs = generate(\n",
    "            model, x, tokenizer, max_len=CFG.generation_steps, top_k=0, top_p=1)\n",
    "        bboxes, labels, confs = postprocess(\n",
    "            batch_preds, batch_confs, tokenizer)\n",
    "        all_bboxes.extend(bboxes)\n",
    "        all_labels.extend(labels)\n",
    "        all_confs.extend(confs)\n",
    "\n",
    "os.mkdir(\"results\")\n",
    "for i, (bboxes, labels, confs) in enumerate(zip(all_bboxes, all_labels, all_confs)):\n",
    "    img_path = img_paths[i]\n",
    "    img = cv2.imread(img_path)[..., ::-1]\n",
    "    img = cv2.resize(img, (CFG.img_size, CFG.img_size))\n",
    "    img = visualize(img, bboxes, labels, id2cls, show=False)\n",
    "\n",
    "    cv2.imwrite(\"results/\" + img_path.split(\"/\")[-1], img[..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcWqMhDkowFM"
   },
   "source": [
    "# Final Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMtFtyWtou17"
   },
   "source": [
    "I hope you've enjoyed this tutorial and learned something new. As always, I will be glad to hear your comments on this tutorial or answer any questions you might have regarding the paper and model.\n",
    "Have a nice day!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
