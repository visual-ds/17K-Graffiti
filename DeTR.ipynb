{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b8318b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bfcf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer-based model architecture\n",
    "class DeTR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # Define the transformer backbone\n",
    "        self.transformer = nn.Transformer(num_encoder_layers=6, num_decoder_layers=6, \n",
    "                                          d_model=256, dim_feedforward=1024, dropout=0.1)\n",
    "        # Define the detection head\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 4) # 4 outputs for bounding box regression\n",
    "        self.fc4 = nn.Linear(64, num_classes) # num_classes outputs for object classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape the input image\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.reshape(b, c, h*w)\n",
    "        # Pass the image through the transformer backbone\n",
    "        x = self.transformer(x, x)\n",
    "        x = x.mean(dim=1) # take the average of the patches\n",
    "        # Pass the features through the detection head\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        bbox_output = self.fc3(x)\n",
    "        class_output = nn.functional.softmax(self.fc4(x), dim=1)\n",
    "        return bbox_output, class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25e79a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2aa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    bbox_outputs, class_outputs = outputs\n",
    "    bbox_targets, class_targets = targets\n",
    "    # Compute the smooth L1 loss for bounding box regression\n",
    "    bbox_loss = nn.functional.smooth_l1_loss(bbox_outputs, bbox_targets)\n",
    "    # Compute the cross-entropy loss for object classification\n",
    "    class_loss = nn.functional.cross_entropy(class_outputs, class_targets)\n",
    "    # Compute the overall loss\n",
    "    loss = bbox_loss + class_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b2e43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        for i, (images, targets) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            bbox_targets, class_targets = targets\n",
    "            bbox_outputs, class_outputs = model(images)\n",
    "            loss = loss_fn((bbox_outputs, class_outputs), (bbox_targets, class_targets))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, 10, i+1, len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce6392fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation function\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_bbox_loss = 0\n",
    "        total_class_loss = 0\n",
    "        total_samples = 0\n",
    "        for images, targets in dataloader:\n",
    "            bbox_targets, class_targets = targets\n",
    "            bbox_outputs, class_outputs = model(images)\n",
    "            bbox_loss = nn.functional.smooth_l1_loss(bbox_outputs, bbox_targets)\n",
    "            class_loss = nn.functional.cross_entropy(class_outputs, class_targets)\n",
    "            total_bbox_loss += bbox_loss.item() * images.size(0)\n",
    "            total_class_loss += class_loss.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "        avg_bbox_loss = total_bbox_loss / total_samples\n",
    "        avg_class_loss = total_class_loss / total_samples\n",
    "    return avg_bbox_loss, avg_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd012739",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b3d4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_538340/335357733.py\", line 1, in <module>\n",
      "    train_dataset = CocoDetection(root='datasets/coco/train2017',\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/torchvision/datasets/coco.py\", line 112, in __init__\n",
      "ModuleNotFoundError: No module named 'pycocotools'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoDetection(root='datasets/coco/train2017',\n",
    "                              annFile='datasets/coco/annotations/instances_train2017.json',)\n",
    "#                               transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeTR()\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a57c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06efe1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function\n",
    "def main():\n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9725c05b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ViT' from 'torchvision.models' (/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/torchvision/models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CocoDetection\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViT\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set up the dataset and data loader\u001b[39;00m\n\u001b[1;32m      9\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     12\u001b[0m ])\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ViT' from 'torchvision.models' (/home/felipe/anaconda3/envs/graffiti/lib/python3.8/site-packages/torchvision/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import ViT\n",
    "\n",
    "# Set up the dataset and data loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Set up the model\n",
    "model = ViT(num_classes=91)\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, 10, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'vit_object_detection.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
